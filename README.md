# ğŸ§  Gemini Benchmark

A comprehensive benchmarking framework for evaluating Google's Gemini multimodal AI models across different tasks and modalities.

## ğŸ“‹ Overview

This framework provides tools to:

1. ğŸ“Š **Benchmark Gemini models** across text, image, and audio modalities
2. ğŸ” **Evaluate model performance** using a variety of metrics
3. ğŸ“ˆ **Compare results** between different models or model versions
4. ğŸ§ª **Process and normalize data** for consistent evaluation

## ğŸš€ Getting Started

### Prerequisites

- Python 3.8+
- Google API key with access to Gemini models

### Installation

1. Clone this repository:
   ```bash
   git clone https://github.com/retrogtx/benchmarking-gemini.git
   cd benchmarking-gemini
   ```

2. Create and activate a virtual environment:
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

4. Set up your environment variables:
   ```bash
   cp .env.example .env
   ```
   Then edit `.env` to add your Google API key and other configuration.

## âš¡ Quick Start Guide

This guide walks through the complete end-to-end process for running the benchmarking system.

### 1. Setup Environment Variables

First, ensure your `.env` file is configured properly:

```bash
# Required
GOOGLE_API_KEY=your_google_api_key_here
MODEL_NAME=gemini-1.0-pro  # Or gemini-1.5-pro if you have quota

# Optional
MAX_OUTPUT_TOKENS=2048
TEMPERATURE=0.2
TOP_P=0.95
TOP_K=40
```

### 2. Generate Debug Data

For testing, generate sample data across different modalities:

```bash
# Generate test data for text and image modalities
python scripts/preprocess.py --modalities text image --splits test --debug

# If you want to generate data for all modalities and splits
python scripts/preprocess.py --modalities text image audio --splits train val test --debug
```

This will create debug data files in the `outputs/processed/` directory.

### 3. Run the Evaluation

Run the evaluation pipeline using the generated debug data:

```bash
# Basic evaluation with text modality only
python evaluation/runner.py --debug --data outputs/processed --split test --modalities text

# Multimodal evaluation with text and image
python evaluation/runner.py --debug --data outputs/processed --split test --modalities text image

# Specify specific metrics to evaluate
python evaluation/runner.py --debug --data outputs/processed --split test --modalities text image --metrics accuracy f1
```

### 4. View Results

The evaluation results are saved in the `outputs/results/` directory as JSON files named with the pattern:
`{model}_{split}_{timestamp}.json`

You can examine the detailed results, or if you've run multiple evaluations, compare them:

```bash
# Compare results from different model runs (if you have multiple result files)
python evaluation/comparer.py --results outputs/results/gemini_test_*.json
```

### 5. Debug with Jupyter Notebook

For interactive debugging and visualization:

```bash
# Start Jupyter notebook
jupyter notebook notebooks/debug_pipeline.ipynb
```

### 6. Using Real Data

When you're ready to use real data instead of debug samples:

1. Place your data files in `benchmark/data/` with the naming pattern `{split}.json` (e.g., `test.json`)
2. Run preprocessing without the `--debug` flag:

```bash
python scripts/preprocess.py --modalities text image --splits test
```

3. Run evaluation on the processed real data:

```bash
python evaluation/runner.py --data outputs/processed --split test --modalities text image
```

## ğŸ“Š Using the Framework

### Data Preparation

1. Place your benchmark data in the `benchmark/data/` directory
2. Run the preprocessing script to normalize and prepare the data:
   ```bash
   python scripts/preprocess.py --modalities text image --splits test
   ```

### Running Benchmarks

Run the evaluation pipeline:

```bash
python evaluation/runner.py --model gemini --split test --metrics accuracy f1 multimodal_reasoning
```

### Comparing Models

Compare results from multiple model runs:

```bash
python evaluation/comparer.py --models gemini-1.0-pro gemini-1.5-pro
```

## ğŸ“ Project Structure

```
benchmarking-gemini/
â”œâ”€â”€ benchmark/                 # Benchmark components
â”‚   â”œâ”€â”€ data/                  # Benchmark datasets
â”‚   â”œâ”€â”€ loaders/               # Data loaders for different modalities
â”‚   â””â”€â”€ metrics/               # Evaluation metrics
â”œâ”€â”€ evaluation/                # Evaluation pipeline
â”‚   â”œâ”€â”€ runner.py              # Main evaluation runner
â”‚   â””â”€â”€ comparer.py            # Tool for comparing results
â”œâ”€â”€ models/                    # Model wrappers
â”‚   â”œâ”€â”€ __init__.py            # Model registry
â”‚   â””â”€â”€ gemini.py              # Gemini model wrapper
â”œâ”€â”€ notebooks/                 # Jupyter notebooks for exploration
â”œâ”€â”€ outputs/                   # Output directories
â”‚   â”œâ”€â”€ processed/             # Processed benchmark data
â”‚   â”œâ”€â”€ results/               # Benchmark results
â”‚   â””â”€â”€ logs/                  # Log files
â”œâ”€â”€ scripts/                   # Utility scripts
â”‚   â””â”€â”€ preprocess.py          # Data preprocessing script
â”œâ”€â”€ .env.example               # Example environment variables
â”œâ”€â”€ requirements.txt           # Project dependencies
â””â”€â”€ README.md                  # This file
```

## ğŸ”§ Configuration

The system can be configured through:

1. **Environment variables** (in `.env` file)
2. **Command-line arguments** to the various scripts
3. **Code-level configuration** for advanced scenarios

Key configuration options:

- `GOOGLE_API_KEY`: Your Google API key
- `MODEL_NAME`: Gemini model to use (e.g., "gemini-1.5-pro")
- `MAX_OUTPUT_TOKENS`: Maximum tokens in generated responses
- `TEMPERATURE`: Temperature for response generation

## ğŸ“Š Supported Metrics

The framework includes several evaluation metrics:

- **Accuracy**: Basic accuracy for text responses
- **F1/Precision/Recall**: For evaluating answers with multiple parts
- **Multimodal Reasoning**: Special metric for evaluating reasoning across modalities
- **Long Context**: Evaluates handling of long context inputs

## ğŸ“„ Supported Data Formats

Data should be in JSON format with the following structure (for now):

```json
[
  {
    "id": "unique_sample_id",
    "input": {
      "text": "Question or instruction text",
      "image_path": "path/to/image.jpg"  // Optional
    },
    "expected_output": "Expected model response",
    "metadata": {
      "source": "dataset_name",
      "difficulty": "easy|medium|hard",
      "category": "category_name"
    }
  }
]
```

## ğŸ“ˆ Example Workflow

1. **Prepare your benchmark data** in JSON format
2. **Run preprocessing** to normalize inputs:
   ```bash
   python scripts/preprocess.py
   ```
3. **Run the benchmark**:
   ```bash
   python evaluation/runner.py --model gemini --split test
   ```
4. **Analyze results** in the `outputs/results/` directory
5. **Compare different models**:
   ```bash
   python evaluation/comparer.py
   ```

## ğŸ” Debugging

For debugging, use the provided notebook:

```bash
jupyter notebook notebooks/debug_pipeline.ipynb
```

This notebook walks through the complete pipeline with a single example for easier debugging.
